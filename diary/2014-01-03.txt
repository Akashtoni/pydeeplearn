Done today:
  mainly coding (few lines of report)
  fixed the backpropagation and actually running some things

  pretty sure some bugs will still be cracking around

  realised there are better ways of doing backprop than gradient descent, documented them and will try to implement them

TODO:
  a lot of clean up to do.
  make more space for generalization
  show the activation functions as classes to be easily implemented
  check the measure on the MNIST data
  implement mini-batch learning (so far everything is online)

  add wake sleep algorithm to see if it performs better with wake sleep for
  generative fine tuning and then backprop for discriminative?
  caching
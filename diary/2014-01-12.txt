Done today:
  looked at the MNIST results over night. For some reason more layers still does not work and I cannot explain that, not sure if it is a ML problem or a SWE problem.
  I can conclude that it has nothign to do with dropout since I tried with and without it.

  It could be I have a too small learning rate for the fine tune phase and information does not go back trough. I am tring more to see.

  Need to fix this by the end of the day.

  it works with 4 layers but not with 5. same exact code.

  TODO: add some tests